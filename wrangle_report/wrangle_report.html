<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.2.1 (456793)"/><meta name="author" content="Lauran"/><meta name="created" content="2018-06-10 13:17:18 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-06-15 03:23:57 +0000"/><title>wrangle_report</title></head><body><div>This document outlines the process of creating the the final clean and tidy data file: <a href="https://www.dropbox.com/s/e4ijvj1ba7qc719/tweets_clean_final.csv?dl=0">https://www.dropbox.com/s/e4ijvj1ba7qc719/tweets_clean_final.csv?dl=0</a>.</div><div><br/></div><div>It is intended to help the reader understand the steps taken to perform data wrangling (gather, assess and clean) in the case the reader wants to reproduce the analysis.</div><div>Like in the lessons, I kept the 3 wrangling phases grouped together in this document and in the Jupyter notebook. However, It’s important to note that in actually performing the wrangling, a lot of iteration happened.</div><div><br/></div><div>Gather:</div><div><span style="font-weight: bold;">3 data sources were gathered:</span></div><ol><li><div>Archived tweets file (supplied): Imported csv to pandas dataframe.</div></li><li><div>Extended attributes for the tweets were downloaded from the Twitter API using the tweepy library and looking up the specific tweet ID’s from #1. Used the api.get_status method:</div></li><ol><li><div>First created a raw json document. <a href="https://www.evernote.com/l/AAXMHEuGHDVFQ6029LQ1AxcvFFawnn6ZLOA">Here</a> is a link to a sample I created for reference.</div></li><li><div>Looped through each line (tweet) in the json document and created a list of dictionaries - one dictionary per tweet containing the tweet_id, retweet_count and favorite_count. This was used to add these retweet_count and favorite_count attributes to the tweet archive.</div></li><li><div>Used list of dicts to construct a pandas dataframe.</div></li></ol><li><div>Programmatically downloaded the image predictions file from Udacity: imported tsv to pandas dataframe.</div></li></ol><div><br/></div><div><br/></div><div><b>Assess:</b></div><div>I decided to do an assessment on each of the files first before creating one simplified file.</div><ol><li><div>Created copies of the dataframes to work on.</div></li><li><div>Used basic assessment methods in python to explore the dataframes: .info(), </div></li><li><div>Also note I used a function I created on my own (get_real_types) to be able to see what’s hiding behind the “object” pointers.</div></li><li><div>Used .isnull().sum() to see which variables had null values.</div></li><li><div>used .head() method to visually inspect data as well.</div></li><li><div>As I did the assessment, I made an <a href="https://www.evernote.com/l/AAXDPGtZASdKDooEW6HQb8nURYlXvk23EYQ">Assessment Task List</a> to refer to once I started the cleaning phase.</div></li></ol><div><br/></div><div><b>Clean:</b></div><div>I completed the cleaning steps in accordance with the <a href="https://www.evernote.com/l/AAXDPGtZASdKDooEW6HQb8nURYlXvk23EYQ">task list</a> referenced above and then created a single data frame based on the archive tweet file.</div><div><br/></div><div><br/></div><div><br/></div></body></html>